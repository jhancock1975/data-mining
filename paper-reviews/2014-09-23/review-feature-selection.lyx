#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}% http://ctan.org/pkg/fancyhdr
\fancyhead{}% Clear all headers
%\fancyfoot{}% Clear all footers
\fancyhead[C]{John Hancock}% Place "John Hancock" in Center of header
\renewcommand{\headrulewidth}{0pt}% Remove header rule
%\renewcommand{\footrulewidth}{0pt}% Remove footer rule
\pagestyle{fancy}% Set page style to "fancy"
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Standard
John Hancock
\end_layout

\begin_layout Standard
Advanced Data Mining and Machine Learning CAP 6778
\end_layout

\begin_layout Standard
September 
\begin_inset Formula $26^{th}$
\end_inset

, 2014
\end_layout

\begin_layout Part*
Review of: Feature Selection with High-Dimensional Imbalanced Data
\end_layout

\begin_layout Standard
The paper starts with precise definition of feature selection, one that
 I have not seen before:
\end_layout

\begin_layout Standard
Given a dataset with features 
\begin_inset Formula \[
\mathcal{F}=\left(X^{1},\ldots,X^{m}\right)\]

\end_inset


\end_layout

\begin_layout Standard
select subset 
\begin_inset Formula \[
\mathcal{F^{\textrm{0}}}=\left(X^{j_{1}},\ldots X^{j_{p}}\right)\]

\end_inset


\end_layout

\begin_layout Standard
such that 
\begin_inset Formula $p<<m$
\end_inset

 and we get something like maximum classifier accuracy using 
\begin_inset Formula $\mathcal{F}^{\textrm{0}}$
\end_inset

.
\end_layout

\begin_layout Standard
The introduction also points out that we have to use feature selection for
 high dimensional data sets, can speed up model training time, reduces over-fitt
ing, use fewer computing resources.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

 for feature selection uses 
\begin_inset Formula $\chi^{2}$
\end_inset

, information gain (IG), gain ratio GR, Relief F, Relief F weigted, symmetric
 uncertainty (SU), and three (in 2009) new threshold based filters: are
 under ROC cuve (AUC), maximum f-measure (FM) maximum geometric mean (GM).
 
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

 the data sampling techniques used are: random under-sampling (RUS), random
 oversampling (ROS), synthetic minority oversampling (SM), Wilson editing
 (WE).
 Recall that in WE we remove noisy instances from majority class.
\end_layout

\begin_layout Standard
Two objectives of the paper: 
\end_layout

\begin_layout Itemize
understand correlation relationships between the 9 filtering techniques
 (which rankers or TBFS techniques give similar lists of attributes)
\end_layout

\begin_layout Itemize
understand whether or not the relationships change when we sample the data
 (do the lists of attributes change before/after sampling).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

uses 6 bioinformatic datasets: all have at least slight imbalanced class
 ratios.
\end_layout

\begin_layout Standard
Section 3 of 
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

, reviews feature rankers: 
\begin_inset Formula $\chi^{2}$
\end_inset

, IG, GR, Relief, Relief F (mentions ReliefF is implemented in Weka), Symmetric
 Uncertainty (SU), 
\end_layout

\begin_layout Standard
Section 4 reviews threshold based filters.
 Explains how to map a feature to (0,1), treat as posterior probability,
 and vary threshold look at performance of classifier for the one attribute,
 and rank attributes accordingly.
 Sections 4.1-4.3 show how to calculate metrics AUC, GM, F-Measure.
\end_layout

\begin_layout Standard
Section 5 of 
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

 reviews sampling techniques used.
 
\end_layout

\begin_layout Standard
A key contribution of 
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

 (Section 6) is to use Kendall’s Tau rank correlation to compare feature
 ranking.
 This technique compares two lists, measures correlation between the two
 lists.
 
\begin_inset Formula $\tau=1$
\end_inset

 for two rankings means they are in complete agreement, 
\begin_inset Formula $\tau=-1$
\end_inset

 for two rankings means the rankings are exactly opposite.
 
\begin_inset Formula $\tau$
\end_inset

 close to 0 means correlation between rankings is weak.
\end_layout

\begin_layout Standard
Results: first table (Figure 1) compares all 9 feature selection techniques,
 table shows which ranking techniques are similar and not similar.
 The top half of the table is average 
\begin_inset Formula $\tau$
\end_inset

 over all 5 data sets for a pair of feature selection techniques.
 The bottom half of Figure 1 is standard deviation of 
\begin_inset Formula $\tau$
\end_inset

 for pars of feature selection techniques.
\end_layout

\begin_layout Standard
The results in Figure 1 show the 3 new techniques F, GM, AUC are not correlated
 with rankers.
\end_layout

\begin_layout Standard
Figure 2: top half of the matrix is correlations between pairs of feature
 selection techniques when we do sampling.
 If correlation is low sampling had an impact on the feature selection technique
 and we cannot assume that, if we are going to sample the data, that techniques
 that are correlated before sampling will still be close or the same after
 sampling.
 Note: Figure 2 is testing the Wilson Editing (WE) sampling technique only.
 Generally in Figure 2, it seems that if techniques are correlated before
 WE , they will be correlated after WE.
 
\end_layout

\begin_layout Standard
Figure 3 is obtained in the same way as Figure 2, but here the sampling
 technique is random oversampling (ROS).
 Generally it shows that correlation does not hold after ROS.
\end_layout

\begin_layout Standard
Table 2 gives us a third section of results.
 The motivation is that data in tables in section two is overwhelming, so
 we use Frobenius norm to calculate distance between correlation matrices
 to get a single number.
 The results in table 2 back up observations about correlations and sampling
 techniques in Figure 1 and Figure 2.
 Table 2 shows no sampling/WE produce similar features - mean of Frobenius
 norm is 0.252.
 For RUS ROS highest, mean of Frobenius norm is 1.509.
 Which means that for any pair of rankers, the lists of attributes we get
 after applying RUS will most likely be very different the list we obtain
 after using ROS.
\end_layout

\begin_layout Standard
Remember - lower Frobenius norm means more similar.
 The numbers in table 2 are Frobenius norm distances, 
\series bold
not 
\series default

\begin_inset Formula $\tau$
\end_inset

 values.
\end_layout

\begin_layout Standard
Conclusions: 
\begin_inset Formula $\chi^{2}$
\end_inset

, IG, GR, SU are highly correlated techniques - they will produce similar
 lists of attributes.
 RF, RFW are moderately correlated.
 AUC, GM, FM (F-measure) - new techniques - are moderately correlated with
 one and other, but not correlated with 6 other feature selection techniques.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"

\end_inset

shows a relationship between data sampling and feature selection.
 Future work needs to investigate the relationships hold for other bioinformatic
 data sets.
 Another future task is to evaluate performance of models that the new feature
 selection techniques produce.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-68"

\end_inset

T.
 M.
 Khoshgoftaar, J.
 Van Hulse, A.
 Napolitano, R.
 Wald.
 
\begin_inset Quotes eld
\end_inset

Feature Selection with High-Dimensional Imbalanced Data,
\begin_inset Quotes erd
\end_inset

 in IEEE International Conference on Data Mining Workshops, © 2009 IEEE,
 doi: 10.1109/ICDMW.2009.35
\end_layout

\end_body
\end_document
