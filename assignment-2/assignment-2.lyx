#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}% http://ctan.org/pkg/fancyhdr
\fancyhead{}% Clear all headers
%\fancyfoot{}% Clear all footers
\fancyhead[C]{John Hancock}% Place "John Hancock" in Center of header
\renewcommand{\headrulewidth}{0pt}% Remove header rule
%\renewcommand{\footrulewidth}{0pt}% Remove footer rule
\pagestyle{fancy}% Set page style to "fancy"
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Assignment 2, Modeling Assignment: Using Meta Learning Schemes with a Strong
 and a Weak Learner for Classification.
\end_layout

\begin_layout Standard
John Hancock
\end_layout

\begin_layout Standard
Florida Atlantic University
\end_layout

\begin_layout Standard
Advanced Data Mining and Machine Learning CAP-6778
\end_layout

\begin_layout Standard
jhancoc4@fau.edu
\end_layout

\begin_layout Section*
Summary 
\end_layout

\begin_layout Standard
In this assignment we use various classifiers in Weka on a biometric dataset.
 We evaluate and analyze the false positive and false negative error rates
 of the classifiers using the same methods we use in part 4 of assignment
 1.
 We use a cost sensitive classifier as our top level classifier.
 We configure the cost sensitive classifier to use meta-learners equipped
 with either the boosting algorithm AdaBoostM1 or the Bagging algorithm
 with two levels of iterations for either meta-learner.
 We then use a strong learner (J48 Decision Tree) or a weak learner (Decision
 Stump) as the base classifier.
 We vary the cost matrix of the top level cost sensitive classifier and
 record Type I and Type II error rates.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
For this work we use the Lymphoma96x4026.arff dataset.
 We apply several of Weka's classifiers to the instances of the dataset.
 
\end_layout

\begin_layout Standard
The classifiers we use are: 
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and J48 
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and Decision Stump
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and J48
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and Decision
 Stump
\end_layout

\begin_layout Standard
Note: the J48 decision tree classifier is also known as C4.5.
\end_layout

\begin_layout Standard
For each classifier we determine the optimal ratio of the cost of a Type
 II error to the cost of a Type I error.
 
\end_layout

\begin_layout Standard
We determine the optimal ratio for two configurations of boosting and bagging.
 In the first configuration, we configure boosting or bagging to do 10 iteration
s.
 In the second configuration we configure boosting or bagging to do 25 iteration
s.
\end_layout

\begin_layout Standard
The Assignment 1 requirements document states, 
\begin_inset Quotes eld
\end_inset

Type I (False Positive): a nonACL module is classified as ACL.
\begin_inset Quotes erd
\end_inset

 Hence, ACL must be the positive class, because this statement says that
 something is classified as positive and it is classified in the ACL class.
 Since ACL must be the positive class, nonACL must be the negative class.
\end_layout

\begin_layout Standard
We used the Weka Java API to automate the running of classifiers on the
 dataset, gathering evaluation data on the results of classification, and
 generate the graphics below.
 The source code for Weka automation is available at 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://github.com/jhancock1975/data-mining-assignment-2/tree/master/cap-6778
\end_layout

\end_inset

.
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Standard
For each classifier we set the Type I error cost to 1.0, and allow the Type
 II error cost to take on the following values: 0.125, 0.5, 1.0, 2.0, 4.0, 8.0,
 16.0, 20.0, 21.0, 21.5, 22.0, 22.5, 23.0, 24.0, 28.0, 31.0 32.0, 34.0, 36.0, 40.0, 64.0,
 128.0.
 These are the same Type I and Type II error costs we use in Assignment
 1.
 We choose these values because assignment 2 states, 
\begin_inset Quotes eld
\end_inset

...vary the cost ratio in the same way as in Part 4 of Assignment I.
\begin_inset Quotes erd
\end_inset

 However, we received feedback from Assignment 1 stating that the optimal
 cost ratio for a cost-sensitive classifier combined with J48 is 31, so
 we decided to add this cost ratio as well.
\end_layout

\begin_layout Standard
Because Type I error cost is held fixed at 1, the previous list of Type
 II error costs is also a list of Type II to Type I cost ratios.
\end_layout

\begin_layout Standard
We used the Lymphoma96x4026.arff dataset as input for various Weka classifiers
 below.
 Lymphoma96x4026.arff has 73 nonACL instances (the negative class) and 23
 ACL instances (the positive class).
 Error rates below are obtained from dividing the number of misclassified
 instances by the number of instances of the positive or negative class
 in a particular class in Lymphoma96x4026.arff.
 The numbers of positive and negative instances are 73 or 23, respectively.
\end_layout

\begin_layout Standard
We report results for 8 different classifiers.
 We have four combinations of classifiers: boosting, bagging, J48 and decision
 stump.
 We then configure boosting and bagging with either 10 or 25 iterations,
 hence 8 classifiers.
 In Weka, we place each of the 8 classifiers in a cost-sensitive classifier,
 where we vary the Type II error costs in the cost-sensitive classifiers
 cost matrix according to the values in the list above.
 There are 22 values in the list, therefore we construct 176 classifiers
 for this work.
 
\end_layout

\begin_layout Standard
In Weka, we did not change any other settings for classifiers.
 
\end_layout

\begin_layout Standard
Results below are grouped according to classifiers modulo the value we used
 for the Type II error cost.
\end_layout

\begin_layout Standard
The data for results is available at 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://github.com/jhancock1975/data-mining/blob/master/assignment-2/cap6778-app/
java-output/
\end_layout

\end_inset

.
\end_layout

\begin_layout Section*
Comparison of Classifiers
\end_layout

\begin_layout Standard
We tabulate the optimal cost ratios along with the false positive and false
 negative rates that we find in parts 1-8.
 The following sections give details on how we determined the optimal cost
 ratios in the table below as well as plots of error rates as the cost ratio
 changes.
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="5">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Classifier
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Iterations
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Optimal Cost Ratio
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FPR
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FNR
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.260
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.217
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging Decision Stump 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.397
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.348
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting J48 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.082
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.217
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting Decision Stump 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.164
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.304
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.301
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.217
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.356
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.260
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Boosting J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
16.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.027
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.217
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.137
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.217
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cost Sensitive J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
n/a
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
31.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.315
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.304
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Table 1: comparison of error rates for optimal cost ratios between all classifie
rs.
 For comparison we add the results for Part 4 of Assignment I.
 FNR stands for false negative rate, and FPR stands for false positive rate.
\end_layout

\begin_layout Standard
The first thing that jumps out at us regarding table 1 is the number 0.217.
 This is the lowest Type II error rate but it occurs 5 times in the table
 for different kinds of classifiers.
 We are skeptical that we get the same number so many times.
 However we remind the reader that in the Lymphoma96x4026.arff dataset we
 have 23 instances that are truly members of the positive class.
 We checked the raw data we have for each classifier and the false negative
 rate is indeed 0.21739130434782608 for each classifier where we report the
 number 0.217 above.
 This corresponds to 5.
 We feel it is a topic for further research that we have 5 false negative
 cases for so many different configurations of classifiers.
 
\end_layout

\begin_layout Section*
Part 1: Cost Sensitive Classifier Combined with Bagging and J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Bagging-J48-10.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 1: Type I and Type II error rates for a cost sensitive classifier
 combined with bagging meta-classifier and J48 base classifier.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
The optimal cost ratio is one where Type I and Type II errors are approximately
 equal with Type II errors as low as possible.
 Figure 1shows that, for this classifier, this is when the ratio attains
 the value 8, corresponding to a false positive rate of 0.260 and a false
 negative rate of 0.217.
\end_layout

\begin_layout Section*
Part 2: Cost Sensitive Classifier Combined with Bagging and Decision Stump
 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different 2x2 cost
 matrices.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
 Other classifier settings in Weka are left at default values.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Bagging-Decision-Stump-10.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 2: Type I and Type II error rates for a cost sensitive classifier
 with bagging using 10 iterations.
 Decision stump is the base classifier.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
Figure 2 indicates that the cost ratio where Type I and II errors are approximat
ely equal with Type II errors as low as possible is 4.
 For this cost ratio, we have a Type I error rate of 0.397 and a Type II
 error rate of 0.348.
\end_layout

\begin_layout Section*
Part 3: Cost Sensitive Classifier Combined with Boosting (AdaBoostM1) and
 J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 10 iterations.
 Otherwise, we use the same methodology as in parts 1 and 2.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Boosting-J48-10.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 3: Type I and Type II error rates for a cost-sensitive classifier
 combined with boosting and J48 decision tree.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
We do not see the same pattern in Figure 3 that we see in figures 1 and
 2.
 For the maximum Type II : Type 1 cost ratio in Figure 3 of 128:1 we see
 the false positive rate is 0.16 and the false negative rate is 0.39.
 We ran this classifier an additional time with a cost ratio of 1024:1.
 The false positive rate for this cost ratio is 0.192 and the false negative
 rate is 0.261.
 The false positive rate is less than the false negative rate, so for the
 1024:1 cost ratio the false positive rate curve does not cross the false
 negative rate curve.
 Therefore the familiar 'X' shape pattern in figures 1 and 2 does not emerge
 for a cost ratio many times larger than the cost ratios we see for the
 classifiers we build in parts 1 and 2.
 
\end_layout

\begin_layout Standard
The error rate curves in Figure 3 do not cross.
 However if one is compelled to demand an optimal cost ratio, we offer the
 ratio where Type II error rates are lowest, which is at a cost ratio of
 24.0 where the false positive rate is 0.082 and the false negative rate is
 0.217.
\end_layout

\begin_layout Section*
Part 4: Cost Sensitive Classifier Combined with Boosting (AdaBoostM1) and
 Decision Stump - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Boosting-Decision-Stump-10.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 4: misclassification rates for a cost-sensitive classifier combined
 with boosting and decision stump.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
Figure 4 is similar to figure 3 in that its error rate curves do not cross.
 We see three cost ratios between 10 and 100 where the the Type II error
 rate reaches a minimum.
 We fell the cost ratio for the second of these minimum Type II error rates,
 where the Type I error rate is at a local minimum is the the optimal cost
 ratio for this classifier.
 At this point, the cost ratio is 20.0, the Type I error rate is 0.164 and
 the Type II error rate is 0.304.
\end_layout

\begin_layout Section*
Part 5: Cost Sensitive Classifier Combined with Bagging and J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 25 iterations.We then run the classifier on the
 input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Bagging-J48-25.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 5: cost-sensitive classifier combined with bagging and J-48, where
 we set bagging to perform 25 iterations.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
Figure 5 shows the 'X' shape we see in parts 1 and 2.
 Here the optimal cost ratio is 8, which corresponds to a false negative
 rate of 0.217 and a false positive rate of 0.301.
 We remind the reader that for the purposes of this study, the optimal cost
 ratio is the cost ratio where Type I and Type II errors are approximately
 equal with Type II errors as low as possible.
\end_layout

\begin_layout Section*
Part 6: Cost sensitive Classifier Combined with Bagging and Decision Stump
 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Bagging-Decision-Stump-25.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 6: Type I and Type II error rates for a cost sensitive classifier
 combined with bagging and decision stump, where bagging is configured to
 do 25 iterations.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
Figure 6 implies that the optimal cost ratio for this classifier is 4.
 At this cost ratio, the false negative error rate is 0.260 and the false
 positive error rate is 0.356.
\end_layout

\begin_layout Section*
Part 7: Cost Sensitive Classifier Combined with Boosting (AdaBoostM1) and
 J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Boosting-J48-25.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 7 error rates for a cost sensitive classifier combined with boosting
 and J48, where boosting is configured with 25 iterations.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
With figure 7, as in parts 3 and 4, it is difficult to identify an error
 cost ratio where Type I errors and Type II errors are approximately equal
 with Type II errors as low as possible, because we have a consistently
 large gap between Type I and Type II errors.
 If forced to choose we would select the point where the Type II error rate
 is at a minimum where the Type I error rate is lowest.
 In figure 6 this is the point where the cost ratio is 16.0, the Type I error
 rate is 0.027 and the Type II error rate is 0.217.
 
\end_layout

\begin_layout Section*
Part 8: Cost sensitive Classifier Combined with Boosting (AdaBoostM1) and
 Decision Stump - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 25 iterations.
 Otherwise our methodology is the same as previous classifier configurations.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Boosting-Decision-Stump-25.ps

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Figure 8: cost sensitive classifier combined with boosting and decision
 stump, boosting configured for 25 iterations.
 FNR stands for false negative rate, and FPR stands for false positive rate.
 False positive rates are based on 73 true positive instances.
 False negative rates are based on 23 true negative instances.
\end_layout

\begin_layout Standard
For determining the optimal cost ratio, we are again in the same situation
 as parts 3, 4, and 7.
 We see two cost ratios between 10 and 100 where the false negative rate
 clearly has the lowest values.
 The error rates we calculate from the classifications that Weka performs
 are actually the same for these two cost ratios.
 The cost ratios for these points are 36.0 and 64.0.
 The false negative rate at these cost ratios is 0.137 and the false positive
 rate is 0.217.
\end_layout

\begin_layout Subsection*
Analysis of Results: ANOVA for Cost Ratios and Number of Iterations Per
 Classifier
\end_layout

\begin_layout Standard
We used the documentation in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-37"

\end_inset

 to do a two factor ANOVA on false positive and false negative error rates
 using R.
 One of the outputs of our Java framework for using Weka are data files
 of cost ratios and error rates.
 For every one of the 8 combinations of classifiers we are required to construct
 in this project, we generate two files of error rates.
 We added the number of iterations given to the boosting or the bagging
 classifier as a categorical variable to these error rates, and then used
 ANOVA functions in R to determine the significance of the number of iterations
 and the cost ratio as factors to predict the error rates of the classifiers.
 This results in 8 ANOVA results.
 The R functions we use to calculate P-values are available in a script
 at 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://github.com/jhancock1975/data-mining/blob/master/assignment-2/cap6778-app/
java-output/do-anova.sh
\end_layout

\end_inset

.
 The table below summarizes the results:
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="5">
<features>
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Classifier
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Error Rate Type
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="1">
<features>
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cost Ratio
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P-Value
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="1">
<features>
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Iterations
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P-Value
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="1">
<features>
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Interaction
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P-Value
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00069
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.84247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99402
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.19
\begin_inset Formula $\times10^{-10}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.928
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.877
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.000504
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.884559
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.977964
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bagging J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8.45
\begin_inset Formula $\times10^{-13}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.585
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.819
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.291
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.408
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.626
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting Decision Stump
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5.38
\begin_inset Formula $\times10^{-7}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.55
\begin_inset Formula $\times10^{-11}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00584
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.720
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.580
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.744
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Boosting J48
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
False Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.40
\begin_inset Formula $\times10^{-5}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0128
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8840
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard

\shape italic
Table 3: P-values for cost ratios, the number of iterations set for boosting
 or bagging meta-classifiers, and their interaction, for cost-sensitive
 classifiers combined with boosting or bagging as meta-classifiers, and
 J48 or Decision Stump as base classifiers
\end_layout

\begin_layout Section*
Conclusions 
\end_layout

\begin_layout Subsection*
How do the results of each classifier compare to the cost-sensitive tree
 obtained in Part 4 of Assignment I?
\end_layout

\begin_layout Standard
Note: it is informative to see Table 1 when considering this question.
 In part 4 of Assignment I, we found an optimal cost ratio of 31.0, with
 a Type II error rate of 0.304, and a Type I error rate of 0.315.
 In Table 1 we see similar error rates for bagging with decision stump where
 we configure bagging with either 10 or 25 iterations.
 These error rates are higher than our best performer, cost-sensitive classifier
 combined with boosting meta-classifier at 25 iterations and J48 decision
 tree base classifier where we have a false positive error rate of 0.027
 and a false negative error rate of 0.217.
\end_layout

\begin_layout Standard
From Table 1 we see 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
that the cost sensitive classifier with a cost ratio of 16.0 combined with
 boosting and J48 decision tree with 25 iterations for boosting 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
gives the overall best classification.
\end_layout

\begin_layout Standard
From Table 3 we can draw the following conclusions from P-values that result
 from ANOVA at a 0.05 significance level:
\end_layout

\begin_layout Itemize
For the cost sensitive classifier combined with bagging and decision stump,
 cost ratio is a significant factor for false positive and false negative
 error rates.
 The number of iterations we set for bagging and its interaction with the
 cost ratio is not significant.
 Note: The graphs of the error rates for these classifiers above form an
 'X' shape for both 10 and 25 iterations.
\end_layout

\begin_layout Itemize
For the cost sensitive classifier combined with bagging and J48 decision
 tree, cost ratio is significant for false positive and false negative error
 rates.
 The number of iterations we set for bagging and its interaction with the
 cost ratio is not significant.
 Note: The graphs of the error rates for these classifiers above form an
 'X' shape for both 10 and 25 iterations.
\end_layout

\begin_layout Itemize
For the cost sensitive classifier combined with boosting and decision stump,
 the cost ratio, the number of iterations, and the interaction between the
 cost ratio and number of iterations are all significant factors for the
 false positive error rate.
 None of the factors or interactions are significant for the false negative
 error rate.
 Note: The graphs of the error rates for these classifiers above do not
 form an 'X' shape for both 10 and 25 iterations.
\end_layout

\begin_layout Itemize
For the cost sensitive classifier combined with boosting and J48 decision
 tree, the two relationships we can glean from the ANOVA results is that
 cost ratio and the number of iterations are significant factors for explaining
 the false positive error rate.
 Other factors and interactions are not significant for the error rates.
 Note: The graphs of the error rates for these classifiers above do not
 form an 'X' shape for both 10 and 25 iterations.
\end_layout

\begin_layout Itemize
For the classifiers we have constructed for Assignment 2, if the graphs
 of error rates form an 'X' shape, cost ratio is a statistically significant
 factor in error rates.
 
\end_layout

\begin_layout Section*
Future Research
\end_layout

\begin_layout Standard
We have developed a Weka automation framework of Java programs and shell
 scripts for running Weka classifiers on datasets and analyzing the significance
 of classifier parameters.
 We are aware of work using random forest classifiers on biometric data.
 We would like to explore extending our Weka automation framework to incorporate
 random forest classifiers.
 We would also like to obtain more biometric data and apply the programs
 in our automation framework to the new data.
 
\end_layout

\begin_layout Standard
As we mention earlier, there the false negative rate 0.217 occurs frequently
 in Table 1.
 We would like to do some research into our results and the dataset to see
 why this rate occurs so frequently.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 5 p.169 fig.
 5.2.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 10 p.379 fig.
 10.6(a).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset


\shape italic
The Area Under an ROC Curve
\shape default
 [Online].
 Available: http://gim.unmc.edu/dxtests/roc3.htm
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-37"

\end_inset

 
\shape italic
Analysis of Variance in R
\shape default
 [Online] Available: http://www.instructables.com/id/Analysis-of-Variance-ANOVA-in
-R/?ALLSTEPS
\end_layout

\end_body
\end_document
