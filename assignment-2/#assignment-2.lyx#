#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}% http://ctan.org/pkg/fancyhdr
\fancyhead{}% Clear all headers
%\fancyfoot{}% Clear all footers
\fancyhead[C]{John Hancock}% Place "John Hancock" in Center of header
\renewcommand{\headrulewidth}{0pt}% Remove header rule
%\renewcommand{\footrulewidth}{0pt}% Remove footer rule
\pagestyle{fancy}% Set page style to "fancy"
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Standard
John Hancock
\end_layout

\begin_layout Standard
Florida Atlantic University
\end_layout

\begin_layout Standard
Advanced Data Mining and Machine Learning
\end_layout

\begin_layout Standard
CAP-6778
\end_layout

\begin_layout Standard
September 30
\begin_inset Formula $^{th}$
\end_inset

, 2014
\end_layout

\begin_layout Part*
\align left
Assignment 2: Modeling assignment: Using meta learning schemes with a strong
 and a weak learner for classification.
\end_layout

\begin_layout Section*
Summary 
\end_layout

\begin_layout Standard
In this assignment we use various classifiers in Weka on a biometric dataset
 to evaluate the performance of the classifiers.
 
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
For this work we use the Lymphoma96x4026.arff dataset.
 We apply several of Weka's classifiers to the instances of the dataset.
 
\end_layout

\begin_layout Standard
The classifiers we use are: 
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and J48
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and Decision Stump
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and J48
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and Decision
 Stump
\end_layout

\begin_layout Standard
For each classifier we determine the optimal ratio of the cost of a Type
 II error to the cost of a Type I error.
 We determine the optimal ratio for two configurations of each classifier.
 In the first configuration, we configure the boosting or the bagging to
 do 10 iterations.
 In the second configuration we configure the boosting or the bagging to
 do 25 iterations.
\end_layout

\begin_layout Standard
The Assignment 1 requirements document states, 
\begin_inset Quotes eld
\end_inset

Type I (False Positive): a nonACL module is classified as ACL.
\begin_inset Quotes erd
\end_inset

 Hence, ACL must be the positive class, because this statement says that
 something is classified as positive and it is classified in the ACL class.
 Since ACL must be the positive class, nonACL must be the negative class.
\end_layout

\begin_layout Section*
Part 1: Cost sensitive classifier combined with bagging and J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 2: Cost sensitive classifier combined with bagging and Decision Stump
 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 3: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 4: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 5: Cost sensitive classifier combined with bagging and J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 25 iterations.We then run the classifier on the
 input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 6: Cost sensitive classifier combined with bagging and Decision Stump
 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 7: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Part 8: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Section*
Comparison of Classifiers
\end_layout

\begin_layout Subsection*
Classifiers with 10 Iterations
\end_layout

\begin_layout Subsection*
Classifiers with 25 Iterations
\end_layout

\begin_layout Subsection*
All Classifiers
\end_layout

\begin_layout Section*
Future Research
\end_layout

\begin_layout Section*
Conclusions
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 5 p.169 fig.
 5.2.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 10 p.379 fig.
 10.6(a).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset


\shape italic
The Area Under an ROC Curve
\shape default
 [Online].
 Available: http://gim.unmc.edu/dxtests/roc3.htm
\end_layout

\end_body
\end_document
