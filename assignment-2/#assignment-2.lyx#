#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}% http://ctan.org/pkg/fancyhdr
\fancyhead{}% Clear all headers
%\fancyfoot{}% Clear all footers
\fancyhead[C]{John Hancock}% Place "John Hancock" in Center of header
\renewcommand{\headrulewidth}{0pt}% Remove header rule
%\renewcommand{\footrulewidth}{0pt}% Remove footer rule
\pagestyle{fancy}% Set page style to "fancy"
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Standard
John Hancock
\end_layout

\begin_layout Standard
Florida Atlantic University
\end_layout

\begin_layout Standard
Advanced Data Mining and Machine Learning
\end_layout

\begin_layout Standard
CAP-6778
\end_layout

\begin_layout Standard
September 30
\begin_inset Formula $^{th}$
\end_inset

, 2014
\end_layout

\begin_layout Part*
\align left
Assignment 2: Modeling assignment: Using meta learning schemes with a strong
 and a weak learner for classification.
\end_layout

\begin_layout Section*
Summary 
\end_layout

\begin_layout Standard
In this assignment we use various classifiers in Weka on a biometric dataset.
 We evaluate and analyze the false positive and false negative error rates
 of the classifiers using the same methods we use in part 4 of assignment
 1.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
For this work we use the Lymphoma96x4026.arff dataset.
 We apply several of Weka's classifiers to the instances of the dataset.
 
\end_layout

\begin_layout Standard
The classifiers we use are: 
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and J48
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with bagging and Decision Stump
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and J48
\end_layout

\begin_layout Itemize
Cost sensitive classifier combined with boosting (AdaBoostM1) and Decision
 Stump
\end_layout

\begin_layout Standard
For each classifier we determine the optimal ratio of the cost of a Type
 II error to the cost of a Type I error.
 
\end_layout

\begin_layout Standard
We determine the optimal ratio for two configurations of boosting and bagging.
 In the first configuration, we configure boosting or bagging to do 10 iteration
s.
 In the second configuration we configure the boosting or the bagging to
 do 25 iterations.
\end_layout

\begin_layout Standard
The Assignment 1 requirements document states, 
\begin_inset Quotes eld
\end_inset

Type I (False Positive): a nonACL module is classified as ACL.
\begin_inset Quotes erd
\end_inset

 Hence, ACL must be the positive class, because this statement says that
 something is classified as positive and it is classified in the ACL class.
 Since ACL must be the positive class, nonACL must be the negative class.
\end_layout

\begin_layout Standard
We used the Weka Java API to automate the running of classifiers on the
 dataset, gathering evaluation data on the results of classification, and
 generate the graphics below.
 The source code for Weka automation is available at https://github.com/jhancock1
975/data-mining-assignment-2.
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Standard
For each classifier we set the Type I error cost to 1.0, and allow the Type
 II error cost to take on the following values: 0.125, 0.5, 1.0, 2.0, 4.0, 8.0,
 16.0, 20.0, 21.0, 21.5, 22.0, 22.5, 23.0, 24.0, 28.0, 32.0, 34.0, 36.0, 40.0, 64.0, 128.0.
 These are the same Type I and Type II error costs we use in Assignment
 1.
 Because Type I error cost is held fixed at I, the previous list of Type
 II error costs is also a list of Type II to Type I cost ratios.
\end_layout

\begin_layout Standard
We used the Lymphoma96x4026.arff dataset as input for various Weka classifieries
 below.
 Lymphoma96x4026.arff has 73 nonACL instances and 23 ACL instances.
 Error rates below are obtained from dividing the number of misclassified
 instances by the number of instances of a particular class in the dataset.
 So, for false positive error rates, the 
\end_layout

\begin_layout Section*
Part 1: Cost sensitive classifier combined with bagging and J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename cap6778-app/Bagging-J48-10.ps

\end_inset


\end_layout

\begin_layout Standard
Figure 1: Type I and Type II error rates for a cost sensitive classifier
 combined with bagging meta-classifier and J48 base classifier.
\end_layout

\begin_layout Standard
The optimal cost ratio is one where Type I and Type II errors are approximately
 equal with Type II errors as low as possible.
 Figure 1 shows this is when the ratio attains the value 8.
 
\end_layout

\begin_layout Section*
Part 2: Cost sensitive classifier combined with bagging and Decision Stump
 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/bagging-decis-stump-10/bagging-decis-stump-10.ps

\end_inset


\end_layout

\begin_layout Section*
Part 3: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48 - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/boosting-j48-10/boosting-j48-10.ps

\end_inset


\end_layout

\begin_layout Section*
Part 4: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump - 10 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 10 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/boosting-decis-stump-10/boosting-decis-stump-10.ps

\end_inset


\end_layout

\begin_layout Section*
Part 5: Cost sensitive classifier combined with bagging and J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and J48.
 We configure bagging to do 25 iterations.We then run the classifier on the
 input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/bagging-j48-25/bagging-j48-25.ps

\end_inset


\end_layout

\begin_layout Section*
Part 6: Cost sensitive classifier combined with bagging and Decision Stump
 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with bagging and decision Stump.
 We configure bagging to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/bagging-decis-stump-25/bagging-decis-stump-25.ps

\end_inset


\end_layout

\begin_layout Section*
Part 7: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48 - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 J48.
 We configure boosting to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/boosting-j48-25/boosting-j48-25.ps

\end_inset


\end_layout

\begin_layout Section*
Part 8: Cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump - 25 Iterations
\end_layout

\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Standard
For this part of the assignment we use Weka to build a classification model
 using a cost sensitive classifier combined with boosting (AdaBoostM1) and
 Decision Stump.
 We configure boosting to do 25 iterations.
 We then run the classifier on the input dataset using different cost ratios.
 Our method for varying the cost ratios is: we set the Type I error cost
 of the cost sensitive classifier to 1 and vary the cost of a Type II error.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Subsubsection*
Misclassification Rates
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename saved-output/boosting-decis-stump-25/boosting-decis-stump-25.ps

\end_inset


\end_layout

\begin_layout Section*
Comparison of Classifiers
\end_layout

\begin_layout Subsection*
Classifiers with 10 Iterations
\end_layout

\begin_layout Subsection*
Classifiers with 25 Iterations
\end_layout

\begin_layout Subsection*
All Classifiers
\end_layout

\begin_layout Section*
Future Research
\end_layout

\begin_layout Section*
Conclusions
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 5 p.169 fig.
 5.2.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

I.
 Witten and E.
 Frank, Data Mining (second edition).
 San Francisco: Elsevier, 2005, ch.
 10 p.379 fig.
 10.6(a).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset


\shape italic
The Area Under an ROC Curve
\shape default
 [Online].
 Available: http://gim.unmc.edu/dxtests/roc3.htm
\end_layout

\end_body
\end_document
