#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{fancyhdr}% http://ctan.org/pkg/fancyhdr
\fancyhead{}% Clear all headers
%\fancyfoot{}% Clear all footers
\fancyhead[C]{John Hancock}% Place "John Hancock" in Center of header
\renewcommand{\headrulewidth}{0pt}% Remove header rule
%\renewcommand{\footrulewidth}{0pt}% Remove footer rule
\pagestyle{fancy}% Set page style to "fancy"
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family rmdefault
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Evaluation and Application of Feature Set Stability Metrics on High Dimensionali
ty Datasets
\end_layout

\begin_layout Standard
John Hancock
\end_layout

\begin_layout Standard
Florida Atlantic University
\end_layout

\begin_layout Standard
Advanced Data Mining and Machine Learning CAP-6778
\end_layout

\begin_layout Standard
jhancoc4@fau.edu
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Standard
This work procedes from the feature selection technique stability metric
 Alelyani 
\shape italic
et.
 al
\shape default
 propose in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

.
 We discuss the evolution of methods for calculating the stability of feature
 selection techniques, attempt to reproduce results, and compare Alelyani
 
\shape italic
et al.'
\shape default
s method for calculating stability to Kuncheva's 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5555"

\end_inset

.
 We provide links to publicly available, Java source code we wrote for calcluati
ng Kuncheva's stability index for a set of sequences of features, as well
 as the source code we wrote for calculating Alelyani 
\shape italic
et al.'
\shape default
s average pairwise similarity.
\end_layout

\begin_layout Part*
Introduction
\end_layout

\begin_layout Section*
Why the Stability of Feature Selection Techniques Matters
\end_layout

\begin_layout Standard
We found it useful to read over 
\begin_inset CommandInset citation
LatexCommand cite
key "key-29"

\end_inset

, to get an idea about why the stability of feature selection techniques
 is important.
 In this work, Ein-Dor 
\shape italic
et al
\shape default
.
 discuss how 70 genes serve as good predictors for patients that would survive
 breast cancer.
 The trouble is that depending on which subset of micro-array data the researche
rs use, they obtain a different set of correlated genes that serves as the
 predictor.
 This is troubling in the sense that we cannot be confident that the set
 of genes a study finds is one that will predict a randomly chosen individual's
 chances for surviving cancer.
 Perhaps that individual has a set of genes that is correlated to those
 found in a study, but none of the particular genes that the study finds.
 Under this scenario, we are likely to classify the individual as a false
 negative.
 It is scenarios such as these that motivate researchers to look for stable
 feature selection techniques for classifier input data.
\end_layout

\begin_layout Section*
Modeling and Stability 
\end_layout

\begin_layout Subsection*
Turney
\end_layout

\begin_layout Standard
The earliest notion of stability as it relates to classifiers that we find
 is from Turney, 
\begin_inset CommandInset citation
LatexCommand cite
key "key-27"

\end_inset

 given in 1995.
 In this work, Turney is interested in decision trees, and the stability
 of features that decision tree algorithms retain as they build classifiers.
 Turney is interested in the stability of learners, and not the stability
 of feature selection techniques.
 In the case of decision trees, the stability of the learner is related
 to the stability of the feature selection technique because feature selection
 is embedded into decision tree algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "key-8"

\end_inset

.
 However, Turney's method of calulating stability involves giving the same
 input to different learners and comparing the output classification that
 the learners produce for the input.
 Turney's approach is not necessarily appropriate for feature selection,
 where we have a discrete set of features to select from for inclusion as
 input values to a classifier.
 
\end_layout

\begin_layout Standard
We believe Turney's work on classifier stability inspires later work on
 feature selection stability.
 For example, it is possible to use a classifier for feature selection 
\begin_inset CommandInset citation
LatexCommand cite
key "key-8"

\end_inset

.
 However, we can use formulas that take into account more aspects of feature
 subsets than what Turney provides for calculating the stability of a learner.
 This work is a survey of the evolution of methods for calculating the stability
 of feature selection techniques.
\end_layout

\begin_layout Subsection*
Dunne 
\shape italic
et al.
\end_layout

\begin_layout Standard
The first example we find of researchers evaluating the stability of feature
 selection techniques is from Dunne 
\shape italic
et al.

\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-1"

\end_inset

, from 2002.
 Dunne 
\shape italic
et al.

\shape default
 propose a method of evaluating the stability of features selection techniques
 based on applying the same feature selection technique to different resampled
 subsets of input data, and evaluating the differences in features selected.
 At this point, we would like to stress the fact that the method Dunne 
\shape italic
et al.

\shape default
 use compares variation between different instances of the output of a feature
 selection algorithm.
 The difference between the approach of Dunne 
\shape italic
et al.

\shape default
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-1"

\end_inset

and the approach of Alelyani 
\shape italic
et al.

\shape default
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 is that Aleyani 
\shape italic
et al.

\shape default
 are taking variation in the input data of the feature selection technique
 into consideration.
\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{H}$
\end_inset

 stability metric prposed in 
\shape italic

\begin_inset CommandInset citation
LatexCommand cite
key "key-1-1"

\end_inset


\shape default
 :
\begin_inset Formula \begin{equation}
\hat{H}=\frac{2}{N\cdot W\cdot\left(W-1\right)}\sum_{i=1}^{W}\sum_{j=i+1}^{W}\sum_{k=1}^{N}\left|m_{ik}-m_{jk}\right|\label{eq:1}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Dunne 
\shape italic
et al.

\shape default
 are interested in improving the stability of wrapper feature selection
 techniques because they believe wrapper based feature selection techniques
 have better performance than filter based feature selection techniques.
 Their goal is to get the better accuracy we have with wrapper based feature
 selection techniques while improving their stability.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-1"

\end_inset

Dunne 
\shape italic
et al
\shape default
.
 are working with relatively small feature sets - sets with between 13 and
 126 attributes.
 A later review of feature selection techniques 
\begin_inset CommandInset citation
LatexCommand cite
key "key-8"

\end_inset

, points out that wrapper based methods are computationally expensive.
 Wrapper based techniques build classifiers to evaluate feature subsets.
 Bioinformatic datasets can have 
\begin_inset Formula $O\left(10^{4}\right)$
\end_inset

features, so in order to apply a wrapper based technique for say, subsets
 of size two, we would need to apply a classifier to 
\begin_inset Formula $O\left(10\right)^{8}$
\end_inset

possible subsets of size two.
 This would cost a significant amount of time, even for classifier running
 times on the order of 
\begin_inset Formula $10^{-2}$
\end_inset

seconds (there are 
\begin_inset Formula $2.6\times10^{6}$
\end_inset

 seconds in a month).
 Later work we find on feature selection statbility metrics involves datasets
 with larger numbers of features; feature sets that are on the order of
 
\begin_inset Formula $10^{4}$
\end_inset

 features.
 Therefore Dunne 
\shape italic
et al.
\shape default
's work is important for providing a methodological framework for evaluation
 the stability of feature selection techniques.
 However, given the size of bioinformatic dataset feature sets, wrapper
 based approaches may be too computationally intensive.
\end_layout

\begin_layout Standard
The feature selection techniques Dunne 
\shape italic
et al
\shape default
.
 use are forward sequential selection (FSS), backward sequential selection
 (BSS) and random hill climbing search (RHC).
\end_layout

\begin_layout Subsection*
A.
 Kalousis 
\shape italic
et al
\shape default
.
\end_layout

\begin_layout Standard
Chronologocially, the next work on the stability of feature selection techniques
 that we find is a work by A.
 Kalousis 
\shape italic
et al
\shape default
.
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

.
 This work investigates the application of three different measures of stability
 using well known feature selection techniques in the Weka tool.
 The authors are interested in the stability of feature selection techniques
 for reasons discussed above; namely that practicioners are not confident
 to assign resources to investigate possible models that are sensitive to
 their input feature sets.
 In their conclusinos, they also pitch their methodology as the first framework
 for evaluating the stability of feature selection techniques.
 The stability metrics the authors use are Pearson's correlation coefficient:
\begin_inset Formula \[
S_{w}\left(w,w^{\prime}\right)=\frac{\sum_{i}\left(w_{i}-\mu_{w}\right)\left(w_{i}^{\prime}-\mu_{w^{\prime}}\right)}{\sqrt{\sum_{i}\left(w_{i}-\mu_{w}\right)^{2}\sum_{i}\left(w_{i}^{\prime}-\mu_{w^{\prime}}\right)^{2}}}\]

\end_inset


\end_layout

\begin_layout Standard
Spearman's rank coefficient:
\begin_inset Formula \[
S_{R}\left(r,r^{\prime}\right)=1-6\sum_{i}\frac{\left(r_{i}-r_{i}^{\prime}\right)^{2}}{m\left(m^{2}-1\right)}\]

\end_inset


\end_layout

\begin_layout Standard
And the Tanimoto distance between two sets:
\begin_inset Formula \begin{equation}
S_{s}\left(s,s^{\prime}\right)=1-\frac{\left|s\right|+\left|s^{\prime}\right|-2\left|s\cap s^{\prime}\right|}{\left|s\right|+\left|s^{\prime}\right|-\left|s\cap s^{\prime}\right|}\label{eq:2}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

 Kalousis et al.
 report that stability results for 
\begin_inset Formula $S_{w}$
\end_inset

 are confusing because they include information about all elements in the
 feature set.
 For 
\begin_inset Formula $S_{w}$
\end_inset

 the 
\begin_inset Formula $w_{i}$
\end_inset

 and 
\begin_inset Formula $w_{i}^{\prime}$
\end_inset

 are weightings that a feature selection algorithm gives to each feature
 in a feature set, and the 
\begin_inset Formula $\mu_{w}$
\end_inset

 and 
\begin_inset Formula $\mu_{w^{\prime}}$
\end_inset

 are mean weights for all features.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

 Kalousis et al.
 also report that trouble with 
\begin_inset Formula $S_{R}$
\end_inset

 because some of the feature selection techniques they use assign equal
 ranks, that is, 
\begin_inset Quotes eld
\end_inset

tie,
\begin_inset Quotes erd
\end_inset

 ranks to many low ranking features.
 So, calculatins for 
\begin_inset Formula $S_{R}$
\end_inset

can also end up including information about more features that a feature
 selection technique actually chooses.
\end_layout

\begin_layout Standard
For these reasons, Kalousis et al.
 prefer the feature selection technique that deals with selected featuer
 subsets exclusively, which is the Tanimoto distance between sets, 
\begin_inset Formula $S_{s}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Kalousis 
\shape italic
et al.

\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

 is also the first paper we reviewed that involves datasets with dimensionality
 similar to the datasets in Alelyani 
\shape italic
et al
\shape default
.
 Kalousis 
\shape italic
et al
\shape default
.
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

 report that they use datasets with 824, 2,200, and 4,928 features, respectively.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

we have one dataset that has 5,748 features.
 However in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 the authors use datasets that are an order of magnitude greater than the
 largest used in Kalousis 
\shape italic
et al
\shape default
.
 
\end_layout

\begin_layout Standard
In the results of 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

, Kalousis 
\shape italic
et al.

\shape default
 the authors write they are more interested in 
\begin_inset Formula $S_{s}$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

 because it involves the output of feature selection techniques, subsets
 of features, rather than 
\begin_inset Formula $S_{R}$
\end_inset

 and 
\begin_inset Formula $S_{w}$
\end_inset

 that look at ranks and weights and involve the entire feature set.
 This is interesting because a few years after Kalousis 
\shape italic
et al
\shape default
.
 publish 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset

, in 2007 Kuncheva 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5555"

\end_inset

 publishes a highly referenced technique for measuring the stability of
 feature selection techniques, that also involves only the features a feature
 selection technique provides.
\end_layout

\begin_layout Subsection*
Kuncheva
\end_layout

\begin_layout Standard
At the time of this writing, a Google Scholar search for 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5555"

\end_inset

 reports 156 citataions.
 We are curious as to a reason for the relative popularity of Kuncheva's
 stability index.
 A similar Google Scholar search for the basis of this term paper, the work
 of Alelyani 
\shape italic
et al
\shape default
.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

, the main topic of which is also a method for calculating the stability
 of a feature selection techniques, reports 7 citations.
 
\end_layout

\begin_layout Standard
In the introduction to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5555"

\end_inset

Kuncheva writes that previous studies by Dunne et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-1"

\end_inset

, and Kalousis 
\shape italic
et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Note: we made sure to check that Kuncheva's references and ours refer to
 the same works.
\end_layout

\end_inset

 
\shape default
pay little attention to, 
\begin_inset Quotes eld
\end_inset

...the stability of the estimate of the critereon value.
\begin_inset Quotes erd
\end_inset

 which we interpret as meaning the stability of the metric for feature selection
 as we change the input values (feature subsets) that we apply stability
 metrics to.
 
\end_layout

\begin_layout Standard
Kuncheva's stability index relies on another metric she introduces in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-5555"

\end_inset

, the consistency index.
 The consistency index is calcualted as
\begin_inset Formula \begin{equation}
I_{C}\left(A,B\right)=\frac{rn-k^{2}}{k\left(n-k\right)}\label{eq:3}\end{equation}

\end_inset

 where 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are subsets of some feature set 
\begin_inset Formula $X$
\end_inset

 , 
\begin_inset Formula $\left|A\cap B\right|=r$
\end_inset

, 
\begin_inset Formula $\left|X\right|=n$
\end_inset

, and 
\begin_inset Formula $\left|A\right|=\left|B\right|=k$
\end_inset

.
 Because the value of 
\begin_inset Formula $I_{C}$
\end_inset

involves the size of the intersection of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 , and it is in the numerator of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"

\end_inset

, feature sets with more elements in common will have a higher consistency
 index value.
 In our opinion, this is why we think Kuncheva writes about the stability
 of the estimate of the criteron value; she is proposing a metric that the
 incorporates a factor for overlap of sets so that the output value will
 be linearly proportional to the size of the overlap.
 
\end_layout

\begin_layout Standard
Perhaps Kuncheva's design of a formula for stability that is itself stabie
 is a reason for the popularity of Kuncheva's stability index.
\end_layout

\begin_layout Standard
We found another reason for the popularity of Kuncheva's stability index
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-2"

\end_inset

by Wang, 
\shape italic
et al
\shape default
.
 There the authors mention that Kuncheva's consistency index, which is the
 main component of the stability index, takes into consideration bias due
 to chance.
 We think that perhaps the authors are referring to what Kuncheva defines
 as the stability index:
\begin_inset Formula \[
\mathcal{I_{\mathit{S}}\left(\mathcal{A\left(\mathit{k}\right)}\right)}=\frac{2}{K\left(K-1\right)}\sum_{i=1}^{K-1}\sum_{j=i+1}^{K}I_{c}\left(S_{i}\left(k\right),S_{j}\left(k\right)\right)\]

\end_inset

 which is a summation of consistency index values.
 It makes sense that 
\begin_inset Formula $\mathcal{I}_{S}$
\end_inset

can handle bias due to chance because if 
\begin_inset Formula $S_{i}$
\end_inset

, 
\begin_inset Formula $S_{j}$
\end_inset

 are randomly selected subsets, quantities that are artifacts of the random
 chance in terms of the size of the intersection if 
\begin_inset Formula $S_{i}$
\end_inset

 and 
\begin_inset Formula $S_{j}$
\end_inset

 will have a tendency to cancel one and other over a summation with a sufficentl
y large number of terms.
\end_layout

\begin_layout Section*
Methodology
\end_layout

\begin_layout Standard
The authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 generate 10 folds of sampled data and each fold has 25% of instances.
 This seems like we end up with very small sets of input instances because
 the datasets have between 85 and 185 instances.
 Our technique of probabilistic noise injection may break down due to such
 a small number of instances.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

 the authors report that they used feature list of size 1% of the total
 number of features, so we do the same.
\end_layout

\begin_layout Part*
Results
\end_layout

\begin_layout Part*
Future Research
\end_layout

\begin_layout Standard
The way the average pairwise similarity sums over the intersections of classes
 in sub-datasets lends itself to the MapReduce framework.
\end_layout

\begin_layout Section*
Questions for Authors
\end_layout

\begin_layout Standard
How big is the size of a sample when they write, 
\begin_inset Quotes eld
\end_inset

In order to do this, we randomly sample X1 from the original dataset X.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
It seems like identical folds will have a non-0 similarity because we sum
 distance between all possible pairs of vectors in folds.
 Since we are comparing all pairs, we will compare non equal vectors, and
 the distances between these vectors will be non-0, and contribute to the
 sum.
\end_layout

\begin_layout Standard
Did you sample with replacement, or without replacement?
\end_layout

\begin_layout Standard
Khoshgoftaar in his lecture on, 
\begin_inset Quotes eld
\end_inset

Overcoming big data challenges,
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-111,key-777"

\end_inset

 spoke about using sampling without replacement when injecting noise into
 micro array data, So we did the same here.
\end_layout

\begin_layout Standard
In figure 3(b) it is not clear what the level of peturbation is.
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Section*
Conclusions 
\end_layout

\begin_layout Section*
Future Research
\end_layout

\begin_layout Standard
Look at distance between centroids of feature sets.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-29"

\end_inset

Ein-Dor 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Outcome Signature Genes in Breast Cancer: is There a Unique Set?
\begin_inset Quotes eld
\end_inset

 in 
\shape italic
Bioinformatics
\shape default
, vol.
 21, no.
 2, Oxford University Press, 2004, pp 171-178.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-27"

\end_inset

P.
 Turney, 
\begin_inset Quotes eld
\end_inset

Technical note: Bias and the Quantification of Stability,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
Machine Learning
\shape default
, vol.
 20, pp.
 23–33, Boston, 1995.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1-1"

\end_inset

K.
 Dunne 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Solutions to Instability Problems with Sequential Wrapper-Based Approaches
 To Feature Selection,
\begin_inset Quotes erd
\end_inset

 in
\shape italic
 Journal of Machine Learning Research
\shape default
, MIT Press, Cambridge, MA, 2002, pp.
 1-22
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-22"

\end_inset

A.
 Kalousis 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Stability of Feature Selection Algorithms,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
Fifth IEEE International Conference on Data Mining (ICDM’05)
\shape default
, Houston, TX, 2005, pp 218-225.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5555"

\end_inset

L.
 Kuncheva, 
\begin_inset Quotes eld
\end_inset

A Stability Index for Feature Selection,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
Proceedings of the 25th IASTED International Multi-Conference
\shape default
, Insbruck, Austria, 2007, 390-395.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Y.
 Saeys 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Robust Feature Selection Using Ensemble Feature Selection Techniques,
\begin_inset Quotes erd
\end_inset

 in ECML PKDD Part II, LNAI 5212, Springer-Verlag, Berlin, 2008, pp.
 313–325.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

A.
 Boulesteix and M.
 Slawski, 
\begin_inset Quotes eld
\end_inset

Stability and Aggregation of Ranked Gene Lists,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
Briefings in Bioinformatics
\shape default
, vol.
 10, Oxford University Press, 2009, pp 556-568.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3-1"

\end_inset

Gulgezen 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Stable and Accurate Feature Selection,
\begin_inset Quotes erd
\end_inset

 in
\shape italic
 ECML PKDD 2009, Part I, LNAI 5781, 
\shape default
Eds.
 JW.
 Buntine
\shape italic
 et al.,
\shape default
 Springer Verlag, Berlin, 2009, pp 455-468.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

S.
 Alelyani 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

A Dilemma in Assessing Stability of Feature Selection Algorithms,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
IEEE International Conf.
 High Performance Computing and Communications
\shape default
, 2011, pp.
 701-707.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

K.
 Gao 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Impact of Data Sampling on Stability of Feature Selection for Software Measureme
nt Data,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
23rd IEEE International Conference on Tools with Artificial Intelligence
\shape default
, 2011, pp.
 1004-1011.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"

\end_inset

H.
 Wang 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Measuring Robustness of Feature Selection Techniques on Software Engineering
 Datasets,
\begin_inset Quotes erd
\end_inset

 in IEEE International Conf.
 Information Reuse Integration (IRI), Las Vegas, NV., 2011, pp.
 309-314.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

R.
 Wald 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

The Effect of Measurement Approach and Noise Level on Gene Selection Stability,
\begin_inset Quotes erd
\end_inset

 in IEEE International Conference on Bioinformatics and Biomedicine (BIBM),
 2012, pp 420-424.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

H.
 Wang 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

A Study of Software Metric Selection Techniques: Stability Analysis and
 Defect Prediction Model Performance,
\begin_inset Quotes erd
\end_inset

 in International Journal on Artificial Intelligence Tools, World Scientific
 Publishing Company, 2012, pp.
 1-23.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7-1"

\end_inset

W.
 Awada 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

The Effect of Number of Iterations on Ensemble Gene Selection,
\begin_inset Quotes erd
\end_inset

 in 11th International Conference on Machine Learning and Applications (ICMLA),
 vol.
 2, 2012, pp 198-203.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

R.
 Wald 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

Comparison of Stability for Different Families of Filter-Based and Wrapper-Based
 Feature Selection,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
12th International Conference on Machine Learning and Applications
\shape default
, Miami, FL, 2013.
 pp 457-464.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"

\end_inset

W.
 Awada 
\shape italic
et al
\shape default
., 
\begin_inset Quotes eld
\end_inset

A Review of the Stability of Feature Selection Techniques for Bioinformatics
 Data,
\begin_inset Quotes erd
\end_inset

 2014.
 Available [online] 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://digitool.fcla.edu/view/action/singleViewer.do?dvs=1414870625593~543&locale=e
n_US&VIEWER_URL=/view/action/singleViewer.do?&DELIVERY_RULE_ID=7&adjacency=N&appl
ication=DIGITOOL-3&frameId=1&usePid1=true&usePid2=true
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"

\end_inset

D.
 Dernoncourt 
\shape italic
et al.
\shape default
,
\shape italic
 
\shape default

\begin_inset Quotes eld
\end_inset

Analysis of feature selection stability on high dimension and small sample
 data,
\begin_inset Quotes erd
\end_inset

 in 
\shape italic
Computational Statistics and Data Analysis
\shape default
, vol.
 71, Elsevier, 2014, pp.
 681-693.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-777"

\end_inset

T.
 Khoshgoftaar, Lecture on 
\begin_inset Quotes eld
\end_inset

Overcoming Big Data Challenges 2013 SEKE Keynote,
\begin_inset Quotes erd
\end_inset

 Presented 2 October, 2014.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-111"

\end_inset

T.
 Khoshgoftaar, 
\begin_inset Quotes eld
\end_inset

Overcoming Big Data Challenges,
\begin_inset Quotes erd
\end_inset

 in Keynote speech, 25th International Conference on Software Engineering
 & Knowledge Engineering
\begin_inset Quotes erd
\end_inset

 2013.
\end_layout

\end_body
\end_document
